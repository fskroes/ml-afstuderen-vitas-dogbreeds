{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('3.8.3')",
   "display_name": "Python 3.8.3 64-bit ('3.8.3')",
   "metadata": {
    "interpreter": {
     "hash": "1f4e2c0e6b1212d76664e3f879b79433e80493f92f3c17be203e46f226bd4404"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import azureml.core\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "auth = InteractiveLoginAuthentication(tenant_id = '7d9fdaba-a83f-43d1-8d9e-877fd1d8b9df')\n",
    "ws = Workspace.from_config(auth = auth)\n",
    "\n",
    "# choose a name for experiment\n",
    "experiment_name = 'ml-dog_breeds-experiment'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Experiment Name'] = experiment.name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ]
  },
  {
   "source": [
    "# Create or Attach existing AmlCompute\n",
    "\n",
    "A compute target is required to execute the Automated ML run. In this tutorial, you create AmlCompute as your training compute resource.\n",
    "\n",
    "Creation of AmlCompute takes approximately 5 minutes.\n",
    "If the AmlCompute with that name is already in your workspace this code will skip the creation process. As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read this article on the default limits and how to request more quota."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"tf-compute-instance\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS12_V2',\n",
    "                                                           max_nodes=6)\n",
    "    compute_target = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "source": [
    "# Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv(\"input/labels.csv\")\n",
    "df_labels.head()"
   ]
  },
  {
   "source": [
    "Add filepath to the DataFrame"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels['filename'] = df_labels.id.map(lambda id: f'input/data/train/{id}.jpg')\n",
    "df_labels.head()"
   ]
  },
  {
   "source": [
    "# Data exploration\n",
    "Show what we are ourself getting self into ..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_labels.describe())\n",
    "print('')   \n",
    "print('----------------------------------------------------')\n",
    "print('')\n",
    "print(df_labels.breed.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = df_labels.breed.unique()\n",
    "print('Unique amount of labels', len(unique_labels))"
   ]
  },
  {
   "source": [
    "We have good news - all breeds are present in train dataset.\n",
    "\n",
    "So lets calculate amount if images for every breed and check most frequent."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_labels = df_labels.groupby(\"breed\").count()\n",
    "gr_labels = gr_labels.rename(columns = {\"id\" : \"count\"})\n",
    "gr_labels = gr_labels.sort_values(\"count\", ascending=False)\n",
    "gr_labels.head(10)\n",
    "# gr_labels.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,20))\n",
    "ax = sns.barplot(x=df_labels.breed.unique()[:10], y=df_labels.breed.value_counts()[:10])\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=df_labels.breed.unique(), y=df_labels.breed.value_counts())\n",
    "ax.set_xticklabels('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Data processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df_labels.filename.values.tolist()[:10]]"
   ]
  },
  {
   "source": [
    "Converting every image to a array of Tensors, with image size 256x256 pixels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "img_pixel = np.array([img_to_array(load_img(img, target_size=(224, 224))) for img in tqdm(df_labels.filename.values.tolist() ) ])"
   ]
  },
  {
   "source": [
    "Converting image labels to auto-hot encoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_label = df_labels.breed\n",
    "img_label = pd.get_dummies(df_labels.breed)\n",
    "img_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_label.to_csv('breed-columns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('original label:', df_labels.breed.values[0])\n",
    "print('index where label occurs in hot-encoding array:', img_label.values[0].argmax())\n",
    "print()\n",
    "\n",
    "print('there will be a 1 where the sample label occurs:')\n",
    "print(img_label.values[0].astype(int))\n",
    "# print(np.where(df_labels.breed.unique == img_label))"
   ]
  },
  {
   "source": [
    "Creating the train numpy arrays (X)=images and (Y)=breed of image, therefore dog"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = img_pixel\n",
    "y = img_label.values\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split arrays or matrices into random train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train = X_train.repeat()\n",
    "print('X_train:', X_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    "\n",
    "# Takes data & label arrays, generates batches of augmented data.\n",
    "train_generator = train_datagen.flow(x=X_train, y=y_train, batch_size=32, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "for image in range(0, 3):\n",
    "    plt.subplot(1, 3, image+1)\n",
    "    for X_batch, Y_batch in train_generator:\n",
    "        image = X_batch[0]\n",
    "        plt.title(img_label.columns[Y_batch[0].argmax()])\n",
    "        plt.imshow(image)\n",
    "        break\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Takes data & label arrays, generates batches of augmented data.\n",
    "validation_generator = validation_datagen.flow(x=X_test, y=y_test, batch_size=32, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "for image in range(0, 3):\n",
    "    plt.subplot(1, 3, image+1)\n",
    "    for X_batch, Y_batch in validation_generator:\n",
    "        image = X_batch[0]\n",
    "        plt.title(img_label.columns[Y_batch[0].argmax()])\n",
    "        plt.imshow(image)\n",
    "        break\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Train on Microsoft Azure Machine Learning\n",
    "\n",
    "With this section of the notebook, we are going to define what and how we are going to train a model on a Azure Machine Learning cloud service. We are going to do this by leveraging the Azure SDK and therefore their capabilities."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset, Datastore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data = dogs_ds.random_split(percentage=0.8, seed=42)\n",
    "label_column_name = 'breed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"primary_metric\": 'average_precision_score_weighted',\n",
    "    \"enable_early_stopping\": True,\n",
    "    \"max_concurrent_iterations\": 2, # This is a limit for testing purpose, please increase it as per cluster size\n",
    "    \"experiment_timeout_hours\": 0.25, # This is a time limit for testing purposes, remove it for real use cases, this will drastically limit ablity to find the best model possible\n",
    "    \"verbosity\": logging.INFO,\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(\n",
    "    task = 'classification',\n",
    "    debug_log = 'automl_errors.log',\n",
    "    compute_target = compute_target,\n",
    "    training_data = training_data,\n",
    "    label_column_name = label_column_name\n",
    ")\n",
    "\n",
    "# automl_config = AutoMLConfig(task='classification', debug_log='automl_errors.log', compute_target=compute_target, X=X_train, y=y_train, X_valid=X_test, y_valid=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(automl_config, show_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results via Azure Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE HERE "
   ]
  },
  {
   "source": [
    "# Train on TensorFlow (locally)\n",
    "\n",
    "For the record. You can also run this in the cloud. But in case you are running this in Azure, I would recommend to run the previous part, there we are levering the Azure Cloud SDK."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Building the model\n",
    "\n",
    "Before we can build the model, there are a few things we need to define:\n",
    "\n",
    "- The input shape (images, in the form of Tensors) to our model\n",
    "- The output shape (image labels, in the form of Tensors) of our model\n",
    "- The URL of the model we want to use (because of Transfer Learning)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup input shape to the model\n",
    "INPUT_SHAPE = [None, 224, 224, 3] # batch, height, width, colour channels\n",
    "\n",
    "# Setup output shape of the model\n",
    "OUTPUT_SHAPE = len(df_labels.breed.unique()) # number of unique labels\n",
    "\n",
    "# Setup model URL from TensorFlow Hub\n",
    "MODEL_URL = \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, ZeroPadding2D, Flatten, Dropout, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_transfer_learning(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):\n",
    "    print(\"Building model with:\", MODEL_URL)\n",
    "\n",
    "    # setup the model layers\n",
    "    model = Sequential([hub.KerasLayer(MODEL_URL), Dense(units=OUTPUT_SHAPE, activation='softmax')])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.build(INPUT_SHAPE)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_scratch(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE):\n",
    "    print(\"Building own made model\")\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (1, 1), activation='relu', input_shape=(224, 224, 3)))\n",
    "\n",
    "    model.add(Conv2D(32,kernel_size=(3,3),activation='relu'))\n",
    "    model.add(ZeroPadding2D(padding=(1,1)))\n",
    "    model.add(Conv2D(32,kernel_size=(3,3),activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(OUTPUT_SHAPE, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model_transfer_learning()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_model = create_model_scratch()\n",
    "own_model.summary()"
   ]
  },
  {
   "source": [
    "# Creating callbacks (things to help our model)\n",
    "We've got a model ready to go but before we train it we'll make some callbacks\n",
    "\n",
    "Callbacks are helper functions a model can use during training to do things such as save a models progres, check a models progress or stop training early if a model stops improving."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Tensorboard Callback\n",
    "\n",
    "TensorBoard helps provide a visual way to monitor the progress of your model during and after training.\n",
    "\n",
    "It can be used directly in a notebook to track the performance measures of a model such as loss and accuracy.\n",
    "\n",
    "To set up a TensorBoard callback and view TensorBoard in a notbook, we need to do three things:\n",
    "\n",
    "1. Load the TensorBoard notebook extension.\n",
    "\n",
    "2. Create a TensorBoard callback which is able to save logs to a directory and pass it to our model's fit() function.\n",
    "\n",
    "3. Visualize the our models trainigs logs using %tensorboard magic function (we'll do this later on)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "# Create a function to build a TensorBoard callback\n",
    "def create_tensorboard_callback():\n",
    "    # Create a log directly for storing TensorBoard logs\n",
    "    logdir = os.path.join('logs', datetime.datetime.now().strftime('%d%m%Y-%H%M%S'))\n",
    "\n",
    "    return tf.keras.callbacks.TensorBoard(logdir)"
   ]
  },
  {
   "source": [
    "## Early Stopping Callback\n",
    "\n",
    "Early stopping helps prevent overfitting by stopping a model when a certain evaluation metric stops improving. If a model trains for too long, it can do so well at finding patterns in a certain dataset that it's not able to use those patterns on another dataset it hasn't seen before (doesn't generalize).\n",
    "\n",
    "It's basically like saying to our model, \"keep finding patterns until the quality of those patterns starts to go down.\"\n",
    "\n",
    "Therefore, when there is not enough change in our model, we can say that the model have reached a equilibrium."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create early stopping (once our model stops improving, stop training)\n",
    "# it stops after 3 rounds of no improvements\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "\n",
    "# Uncomment if learning rate reductions to model needs to be added.\n",
    "# Also add it the the model.fit_generator function.\n",
    "# learning_rate_reduction = ReduceLROnPlateau(\n",
    "#     monitor='val_accuracy', \n",
    "#     patience=2, \n",
    "#     verbose=1, \n",
    "#     factor=0.5, \n",
    "#     min_lr=0.0001\n",
    "# )\n",
    "\n",
    "# callbacks = [early_stopping, learning_rate_reduction]"
   ]
  },
  {
   "source": [
    "## Save checkpoints during training\n",
    "\n",
    "You can use a trained model without having to retrain it, or pick-up training where you left off in case the training process was interrupted. The tf.keras.callbacks.ModelCheckpoint callback allows you to continually save the model both during and at the end of training.\n",
    "\n",
    "Saves every 5 epoch a checkpoint"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1, save_freq=5)\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "model.save_weights(checkpoint_path.format(epoch=0))"
   ]
  },
  {
   "source": [
    "## Load weigts from checkpoint"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "print(latest)\n",
    "\n",
    "# Loads the weights\n",
    "model.load_weights(latest)"
   ]
  },
  {
   "source": [
    "## Training a model\n",
    "\n",
    "The final parameter we'll define before training is NUM_EPOCHS (also known as number of epochs).\n",
    "\n",
    "NUM_EPOCHS defines how many passes of the data we'd like our model to do. A pass is equivalent to our model trying to find patterns in each dog image and see which patterns relate to each label.\n",
    "\n",
    "If NUM_EPOCHS=1, the model will only look at the data once and will probably score badly because it hasn't a chance to correct itself. It would be like you competing in the international hill descent championships and your friend Adam only being able to give you 1 single instruction to get down the hill.\n",
    "\n",
    "What's a good value for NUM_EPOCHS?\n",
    "\n",
    "This one is hard to say. 10 could be a good start but so could 100. This is one of the reasons we created an early stopping callback. Having early stopping setup means if we set NUM_EPOCHS to 100 but our model stops improving after 22 epochs, it'll stop training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rounds should we get the model to look through the data?\n",
    "NUM_EPOCHS = 15"
   ]
  },
  {
   "source": [
    "Create a model using create_model().\n",
    "\n",
    "Setup a TensorBoard callback using create_tensorboard_callback() (we do this here so it creates a log directory of the current date and time).\n",
    "\n",
    "Call the fit() function on our model passing it the training data, validatation data, number of epochs to train for and the callbacks we'd like to use.\n",
    "\n",
    "Return the fitted model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Trains a given model and returns the trained version\n",
    "\n",
    "    model = create_model_transfer_learning()\n",
    "\n",
    "    tensorboard = create_tensorboard_callback() # Change this function if you want to use own-model (not transfer learning)\n",
    "\n",
    "    model.fit_generator(\n",
    "        generator=train_generator, \n",
    "        # steps_per_epoch= X_train.shape[0] // 32, \n",
    "        validation_data=validation_generator, \n",
    "        validation_freq=1,  \n",
    "        # validation_steps= y_train.shape[0] // 32,\n",
    "        epochs=NUM_EPOCHS, \n",
    "        verbose=1,\n",
    "        callbacks=[tensorboard, early_stopping, cp_callback])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = train_model()"
   ]
  },
  {
   "source": [
    "# Save model or Load model\n",
    "\n",
    "Call model.save to save a model's architecture, weights, and training configuration in a single file/folder. This allows you to export a model so it can be used without access to the original Python code*. Since the optimizer-state is recovered, you can resume training from exactly where you left off."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete model as new format.\n",
    "# model.save('saved_model/my_model')\n",
    "\n",
    "# Save model as old Keras format. H5 format.\n",
    "# model.save('saved_model/h5_model.h5')\n",
    "\n",
    "# Save model as json format. (Use this for Streamlit)\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"saved_model/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the exact same model, including its weights and the optimizer\n",
    "model = tf.keras.models.load_model('saved_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Checking the TensorBoard logs\n",
    "\n",
    "Now our model has been trained, we can make its performance visual by checking the TensorBoard logs.\n",
    "\n",
    "The TensorBoard magic function (%tensorboard) will access the logs directory we created earlier and viualize its contents.\n",
    "\n",
    "\n",
    "Thanks to our early_stopping callback, the model stopped training after 26 or so epochs (in my case, yours might be slightly different). This is because the validation accuracy failed to improve for 3 epochs.\n",
    "\n",
    "But the good new is, we can definitely see our model is learning something. The validation accuracy got to 65% in only a few minutes.\n",
    "\n",
    "This means, if we were to scale up the number of images, hopefully we'd see the accuracy increase.\n",
    "\n",
    "To see the logs visit : http://localhost:6006 in your browser"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "source": [
    "# Making and evaluating predictions using a trained model\n",
    "\n",
    "Before we scale up and train on more data, let's see some other ways we can evaluate our model. Because although accuracy is a pretty good indicator of how our model is doing, it would be even better if we could could see it in action.\n",
    "\n",
    "Making predictions with a trained model is as calling predict() on it and passing it data in the same format the model was trained on."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the validation data (not used to train on)\n",
    "predictions = model.predict(validation_generator, verbose=1) # verbose shows us how long there is to go\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of predictions\n",
    "predictions.shape"
   ]
  },
  {
   "source": [
    "Making predictions with our model returns an array with a different value for each label.\n",
    "\n",
    "In this case, making predictions on the validation data (2045 images) returns an array (predictions) of arrays, each containing 120 different values (one for each unique dog breed).\n",
    "\n",
    "These different values are the probabilities or the likelihood the model has predicted a certain image being a certain breed of dog. The higher the value, the more likely the model thinks a given image is a specific breed of dog.\n",
    "\n",
    "Let's see how we'd convert an array of probabilities into an actual label.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First prediction\n",
    "# print(predictions[0])\n",
    "print(f'Max value (probability of prediction): {np.max(predictions[0])}')\n",
    "print(f'Sum: {np.sum(predictions[0])}')                                     # because we used softmax activation in our model, this will be close to 1\n",
    "print(f'Max index: {np.argmax(predictions[0])}')                            # the index of where the max value in predictions[0] occurs\n",
    "print(f'Predicted label: {img_label.columns[np.argmax(predictions[0])]}')   # the predicted label"
   ]
  },
  {
   "source": [
    "Having this information is great but it would be even better if we could compare a prediction to its true label and original image.\n",
    "\n",
    "To help us, let's first build a little function to convert prediction probabilities into predicted labels.\n",
    "\n",
    "Note: Prediction probabilities are also known as confidence levels.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn predictions probabilities into their respective label (this is then easier to understand)\n",
    "def get_pred_label(prediction_probabilities):\n",
    "    \"\"\"\n",
    "    Turns an array of prediction probabilities into a label.\n",
    "    \"\"\"\n",
    "\n",
    "    return img_label.columns[np.argmax(prediction_probabilities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a predicted label based on an array of prediction probabilities\n",
    "pred_label = get_pred_label(predictions[0])\n",
    "pred_label"
   ]
  },
  {
   "source": [
    "Now we've got a list of all different predictions our model has made, we'll do the same for the validation images and validation labels.\n",
    "\n",
    "Remember, the model hasn't trained on the validation data, during the fit() function, it only used the validation data to evaluate itself. So we can use the validation images to visually compare our models predictions with the validation labels."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(lambda: validation_generator, (tf.float32, tf.float32))\n",
    "dataset = dataset.unbatch()\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "d = list(dataset.take(1000).as_numpy_iterator())\n",
    "for image, label in d:\n",
    "    images.append(image)\n",
    "    labels.append(get_pred_label(label))\n",
    "\n",
    "# for i, l in list(dataset.as_numpy_iterator()):\n",
    "    # print(i, l)"
   ]
  },
  {
   "source": [
    "We got ways to get:\n",
    "\n",
    "- Prediction labels\n",
    "- Validation labels (truth labels)\n",
    "- Validation images\n",
    "\n",
    "More specifically, we want to be able to view an image, its predicted label and its actual label (true label).\n",
    "\n",
    "The first function we'll create will:\n",
    "- Take an array of prediction probabilities, an array of truth labels, an array of images and an integer.\n",
    "- Convert the prediction probabilities to a predicted label.\n",
    "- Plot the predicted label, its predicted probability, the truth label and target image on a single plot."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred(prediction_probabilities, labels, images, n=1):\n",
    "    # show the prediction, ground truth label and image for sample n\n",
    "\n",
    "    pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n]\n",
    "    # print(pred_prob, true_label, image)\n",
    "\n",
    "    # Get the predicted label\n",
    "    pred_label = get_pred_label(pred_prob)\n",
    "\n",
    "    # Plot image & remove ticks\n",
    "    plt.imshow(image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    # Change the color of the title depending on if the prediction is right or wrong\n",
    "    if pred_label == true_label:\n",
    "        color = \"green\"\n",
    "    else:\n",
    "        color = \"red\"\n",
    "\n",
    "    plt.title(\"{} {:2.0f}% ({})\".format(pred_label, np.max(pred_prob)*100, true_label), color=color)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View an example prediction, original image and truth label\n",
    "plot_pred(prediction_probabilities=predictions, labels=labels, images=images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_conf(prediction_probabilities, labels, n=1):\n",
    "  \"\"\"\n",
    "  Plots the top 10 highest prediction confidences along with\n",
    "  the truth label for sample n.\n",
    "  \"\"\"\n",
    "  pred_prob, true_label = prediction_probabilities[n], labels[n]\n",
    "\n",
    "  # Get the predicted label\n",
    "  pred_label = get_pred_label(pred_prob)\n",
    "\n",
    "  # Find the top 10 prediction confidence indexes\n",
    "  top_10_pred_indexes = pred_prob.argsort()[-10:][::-1]\n",
    "  # Find the top 10 prediction confidence values\n",
    "  top_10_pred_values = pred_prob[top_10_pred_indexes]\n",
    "  # Find the top 10 prediction labels\n",
    "  top_10_pred_labels =  unique_labels[top_10_pred_indexes]\n",
    "\n",
    "  # Setup plot\n",
    "  top_plot = plt.bar(np.arange(len(top_10_pred_labels)), top_10_pred_values, color=\"grey\")\n",
    "  plt.xticks(np.arange(len(top_10_pred_labels)), labels=top_10_pred_labels, rotation=\"vertical\")\n",
    "\n",
    "  # Change color of true label\n",
    "  if np.isin(true_label, top_10_pred_labels):\n",
    "    top_plot[np.argmax(top_10_pred_labels == true_label)].set_color(\"green\")\n",
    "  else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_conf(prediction_probabilities=predictions, labels=labels, n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check a few predictions and their different values\n",
    "i_multiplier = 0\n",
    "num_rows = 3\n",
    "num_cols = 2\n",
    "num_images = num_rows*num_cols\n",
    "\n",
    "plt.figure(figsize=(5*2*num_cols, 5*num_rows))\n",
    "for i in range(num_images):\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "  plot_pred(prediction_probabilities=predictions, labels=labels, images=images, n=i+i_multiplier)\n",
    "  \n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "  plot_pred_conf(prediction_probabilities=predictions, labels=labels, n=i+i_multiplier)\n",
    "\n",
    "plt.tight_layout(h_pad=1.0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Making predictions on the test dataset\n",
    "\n",
    "To make predictions on the test data, we'll:\n",
    "\n",
    "    Get the test image filenames.\n",
    "    Make a predictions array by passing the test data to the predict() function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'input/data/test/'\n",
    "test_filenames = [test_path + fname for fname in os.listdir(test_path)]\n",
    "\n",
    "test_filenames[:3]"
   ]
  },
  {
   "source": [
    "## How many test images are there?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "def process_image(image_path):\n",
    "  \"\"\"\n",
    "  Takes an image file path and turns it into a Tensor.\n",
    "  \"\"\"\n",
    "  # Read in image file\n",
    "  image = tf.io.read_file(image_path)\n",
    "  # Turn the jpeg image into numerical Tensor with 3 colour channels (Red, Green, Blue)\n",
    "  image = tf.image.decode_jpeg(image, channels=3)\n",
    "  # Convert the colour channel values from 0-225 values to 0-1 values\n",
    "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "  # Resize the image to our desired size (224, 244)\n",
    "  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data\n",
    "print(\"Creating test data batches...\")\n",
    "data = tf.data.Dataset.from_tensor_slices((tf.constant(test_filenames))) # only filepaths\n",
    "data_batch = data.map(process_image).batch(32)\n",
    "data_batch"
   ]
  },
  {
   "source": [
    "Note: Since there are 10,000+ test images, making predictions could take a while, even on a GPU. So beware running the cell below may take up to an hour."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_batch = data_batch\n",
    "# Make predictions on test data batch using the loaded full model\n",
    "test_predictions = model.predict(test_data_batch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 10 predictions\n",
    "test_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred_label(test_predictions[0])\n",
    "test_filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(load_img(test_filenames[0]))\n",
    "plt.title(get_pred_label(test_predictions[0]))"
   ]
  },
  {
   "source": [
    "# Preparing test dataset predictions for CVS export"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To get the data in this format, we'll:\n",
    "\n",
    "- Create a pandas DataFrame with an ID column as well as a column for each dog breed.\n",
    "- Add data to the ID column by extracting the test image ID's from their filepaths.\n",
    "- Add data (the prediction probabilities) to each of the dog breed columns using the unique_breeds list and the test_predictions list.\n",
    "- Export the DataFrame as a CSV to submit it.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas DataFrame with empty columns\n",
    "preds_df = pd.DataFrame(columns=[\"id\"] + list(unique_labels))\n",
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append test image ID's to predictions DataFrame\n",
    "preds_df[\"id\"] = [os.path.splitext(path)[0] for path in os.listdir(test_path)]\n",
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the prediction probabilities to each dog breed column\n",
    "preds_df[list(unique_labels)] = test_predictions\n",
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df.to_csv(\"submission_with_mobilienetV2.csv\", index=False)"
   ]
  }
 ]
}